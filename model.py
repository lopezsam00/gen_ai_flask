from langchain_ibm import WatsonxLLM
from langchain_ibm import ChatWatsonx
from langchain_core.prompts import PromptTemplate
from config import PARAMETERS, LLAMA_MODEL_ID, GRANITE_MODEL_ID, MISTRAL_MODEL_ID, URL, PROJECT_ID, API_KEY
from pydantic import BaseModel, Field
from langchain_core.output_parsers import JsonOutputParser


# Define JSON output structure
class AIResponse(BaseModel):
    summary: str = Field(description="Summary of the user's message")
    sentiment: int = Field(description="Sentiment score from 0 (negative) to 100 (positive)")
    category: str = Field(description="Category of the inquiry (e.g., billing, technical, general)")
    action: str = Field(description="Recommended action for the support rep")

# JSON output parser
json_parser = JsonOutputParser(pydantic_object=AIResponse)

# Function to initialize a model
def initialize_model(model_id):
    return ChatWatsonx(
        model_id=model_id,
        url=URL,
        project_id=PROJECT_ID,
        params=PARAMETERS,
        apikey=API_KEY
    )


# Initialize models
llama_llm = initialize_model(LLAMA_MODEL_ID)
granite_llm = initialize_model(GRANITE_MODEL_ID)
mistral_llm = initialize_model(MISTRAL_MODEL_ID)

# Prompt template

# Helper to append format instructions to the prompt
def add_format_instructions(template: str) -> str:
    return template + "\nRespond in the following JSON format:\n{format_instructions}"

llama_template = PromptTemplate(
    template=add_format_instructions('''<|begin_of_text|><|start_header_id|>system<|end_header_id|>
{system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>
{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>
'''),
    input_variables=["system_prompt", "user_prompt", "format_instructions"]
)

granite_template = PromptTemplate(
    template=add_format_instructions("<|system|>{system_prompt}\n<|user|>{user_prompt}\n<|assistant|>"),
    input_variables=["system_prompt", "user_prompt", "format_instructions"]
)

mistral_template = PromptTemplate(
    template=add_format_instructions("<s>[INST]{system_prompt}\n{user_prompt}[/INST]"),
    input_variables=["system_prompt", "user_prompt", "format_instructions"]
)
def get_ai_response(model, template, system_prompt, user_prompt):
    chain = template | model | json_parser
    return chain.invoke({
        'system_prompt': system_prompt,
        'user_prompt': user_prompt,
        'format_instructions': json_parser.get_format_instructions()
    })

# Model-specific response functions
def llama_response(system_prompt, user_prompt):
    return get_ai_response(llama_llm, llama_template, system_prompt, user_prompt)

def granite_response(system_prompt, user_prompt):
    return get_ai_response(granite_llm, granite_template, system_prompt, user_prompt)

def mistral_response(system_prompt, user_prompt):
    return get_ai_response(mistral_llm, mistral_template, system_prompt, user_prompt)

